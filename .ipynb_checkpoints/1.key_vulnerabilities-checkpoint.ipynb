{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed99ea75",
   "metadata": {},
   "source": [
    "## Key Vulnerabilities in LLM Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9587e12",
   "metadata": {},
   "source": [
    "The field of LLM comprises many benchmarks such as `ARC`, `Hellaswag`, `MMLU`. It's important to clarify that **Benchmarking** $\\ne$ **`Safety & Security`** We should understand that generally, benchmarks:\n",
    "\n",
    "* **Don't test for safety & security:** Thus issues like detecting bias, propagating stereotypes and use of LLMs for malicious or offensive contents are not expressly evaluated.\n",
    "\n",
    "When it comes to evaluating LLM safety, there's no one-size-fits-all solution, and we need to identify specific scenarios to secure against.\n",
    "\n",
    "## What Could Go Wrong?\n",
    "\n",
    "That's the key question to ask given the interested scenario or the specific LLM application.\n",
    "\n",
    "Some resources worth consulting may include:\n",
    "* The OWASP Top 10 LLM applications\n",
    "* AI Incident Database\n",
    "* The AVID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d1208",
   "metadata": {},
   "source": [
    "For the initial analysis, I will be looking at the following common LLM vulnerabilities:\n",
    "* Bias & Stereotypes\n",
    "* Sensitive Information Disclosure\n",
    "* Service Disruptions\n",
    "* Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9022375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
